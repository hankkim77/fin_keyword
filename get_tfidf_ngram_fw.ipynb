{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510f61d7-dbeb-40dd-be84-f0280ba6158b",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9bd2698-0ab9-4102-b630-3ce63fb2b986",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 12:55:43.800605: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-08-23 12:55:43.800645: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import jaydebeapi\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import regex as re\n",
    "from bs4 import BeautifulSoup\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import konlpy\n",
    "from konlpy.tag import Mecab, Okt, Kkma\n",
    "import sklearn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from n_gram_utils import *\n",
    "import sys\n",
    "\n",
    "import os \n",
    "\n",
    "nhlm_lib_path = '/home/work/EDA/NHLM'\n",
    "if nhlm_lib_path not in sys.path:\n",
    "    sys.path.append(nhlm_lib_path)\n",
    "\n",
    "current_dir = os.getcwd()\n",
    "NHLM_dir = current_dir.split('nhlm_app')[0] + 'NHLM/'\n",
    "\n",
    "if NHLM_dir not in sys.path:\n",
    "    sys.path.append(NHLM_dir)\n",
    "    \n",
    "import nhlm\n",
    "from nhlm.function import spam\n",
    "from nhlm.preparation import newsSpamFilter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fb5f206-2a04-4f64-9d47-55b9789b43d5",
   "metadata": {},
   "source": [
    "## Get Weekly N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6cfb648e-c9c3-4410-85b0-5a6571fc7fd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_csv('final_preprocessed.csv')\n",
    "# # df['n_content_cleaned'] = df['n_content_cleaned'].apply(lambda k: eval(''.join(k)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11e17af0-ba75-46d0-b9f5-79d6d18a6086",
   "metadata": {},
   "source": [
    "### Data Loading & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a0e3b2b8-a261-4a3d-9d1a-ac487f6faca2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOADING DATA ...\n",
      "----- DATA LOADED -----\n",
      "test data loaded and embedded successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-23 12:55:55.332184: E tensorflow/stream_executor/cuda/cuda_driver.cc:271] failed call to cuInit: UNKNOWN ERROR (-1)\n",
      "2022-08-23 12:55:55.332237: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ws-78-5b86768658-mbtb9): /proc/driver/nvidia/version does not exist\n",
      "2022-08-23 12:55:55.332489: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "272/272 [==============================] - 118s 431ms/step\n",
      "prediction complete | spam: 0, news: 1\n",
      "DETECTING SPAM NEWS ...\n",
      "----- SPAM NEWS DETECTED -----\n"
     ]
    }
   ],
   "source": [
    "df = spam.get_news_data(start_date = '20220801', end_date = '20220807')\n",
    "print('LOADING DATA ...')\n",
    "df = spam.preprocess_news_data(df)\n",
    "\n",
    "## simple preprocessing for news data\n",
    "\n",
    "df = df[df['n_code'] != '000000  ']  # get rid of news without code\n",
    "df = df.reset_index(drop = True) \n",
    "df['len'] = df['n_content_cleaned'].str.len()  # check news content length\n",
    "df = df[df['len'] > 250]  # get rid of news with short length(250)\n",
    "df = df.reset_index(drop = True) \n",
    "\n",
    "print('----- DATA LOADED -----')\n",
    "\n",
    "## Detect spam news by using 'spam filter'\n",
    "\n",
    "labeler = spam.classifier()\n",
    "labeler.load_data(df)\n",
    "labeler.load_spam_model()\n",
    "labeler.predict_label()\n",
    "print('DETECTING SPAM NEWS ...')\n",
    "\n",
    "print('----- SPAM NEWS DETECTED -----')\n",
    "\n",
    "## get rid of spam tagged news\n",
    "\n",
    "spams = list(labeler.prediction_label)\n",
    "df['spam'] = spams\n",
    "df = df[df['spam'] == 1]\n",
    "df = df.reset_index(drop = True)\n",
    "\n",
    "\n",
    "# tokenize sentences\n",
    "tokenized, pos = tokenize_and_postag(df, 'n_content_cleaned') # return token and pos tag for each 음절 \n",
    "df['tokenized_content'] = tokenized\n",
    "df['pos_tagged_content'] = pos\n",
    "\n",
    "## processing 'stop words' and 'pos'\n",
    "\n",
    "sw = pd.read_csv('sw.txt', delimiter = '\\n', header = None)\n",
    "sw = list(sw[0])\n",
    "cleans = clean_word(df, 'pos_tagged_content', sw) ##get rid of 'stop words' and some 'pos' tag\n",
    "doc_list = get_doc_list(df, cleans)\n",
    "\n",
    "print('----- DATA PREPROCESSED -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ca475634-41ca-4a30-bfa8-74b75e179377",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GETTING FREQUENT N-GRAMS ...\n",
      "----- FREQUENT N-GRAMS EARNED-----\n"
     ]
    }
   ],
   "source": [
    "print('GETTING FREQUENT N-GRAMS ...')\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 1000, ngram_range = (4, 4))\n",
    "tri_tfidf = get_tfidf_ngram(tfidf, df, cleans, doc_list, 0.8)\n",
    "df['tri_tfidf'] = tri_tfidf\n",
    "\n",
    "first_week = get_most_frequent(df, 'tri_tfidf', 50)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 1000, ngram_range = (5, 5))\n",
    "tri_tfidf = get_tfidf_ngram(tfidf, df, cleans, doc_list, 0.8)\n",
    "df['tri_tfidf'] = tri_tfidf\n",
    "\n",
    "first_week2 = get_most_frequent(df, 'tri_tfidf', 50)\n",
    "\n",
    "first_week3 = first_week + first_week2 \n",
    "print('----- FREQUENT N-GRAMS EARNED-----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ca9f896-f7ba-4c5e-8f25-5903b984a103",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- HERE ARE THE FREQUENT N-GRAMS -----\n",
      "\n",
      "\n",
      "1번 이슈: 환경 지배 구조 경영\n",
      "2번 이슈: 다중접속 역할 수행 게임\n",
      "3번 이슈: 미래 성장 동력 확보\n",
      "4번 이슈: 플로리다주 케이프커내버럴 우주 기지\n",
      "5번 이슈: 연결 잠정 집계 매출\n",
      "6번 이슈: 신성 엔지 관계자 반도체\n",
      "7번 이슈: 미래 자산 운용\n",
      "8번 이슈: 투자자 대상 수요 예측\n",
      "9번 이슈: 치료제 후보 물질 임상\n",
      "10번 이슈: 펠로시 하원 대만 방문\n",
      "11번 이슈: 거리 두기 해제 이후\n",
      "12번 이슈: 사브리 야콥 총리 대만\n",
      "13번 이슈: 반도체 부품 수급 차질\n",
      "14번 이슈: 이선호 제일제당 경영 리더\n",
      "15번 이슈: 자이언트 스텝 포인트 인상\n",
      "16번 이슈: 비상 경영 체제 돌입\n",
      "17번 이슈: 대우 조선 해양 건설\n",
      "18번 이슈: 이동 저장 처리 관리\n",
      "19번 이슈: 노인 주야간 보호 센터\n",
      "20번 이슈: 기반 신용 서비스 활용\n",
      "21번 이슈: 무선 가입자 보유 세계\n",
      "22번 이슈: 브로커 결심 영화제 수상\n",
      "23번 이슈: 김남선 네이버 최고 재무\n",
      "24번 이슈: 러시아 우크라 침공 이후\n",
      "25번 이슈: 코로나 치료제 백신 신약\n",
      "26번 이슈: 시언 수소 전기 트럭\n",
      "27번 이슈: 삼성전자 패밀리 허브 냉장고\n",
      "28번 이슈: 안전 통합 관리 시스템\n",
      "29번 이슈: 교량 하부 구조 공법\n",
      "30번 이슈: 웨어 러블 기기 적용\n",
      "31번 이슈: 연재물 특징 제약 주목\n",
      "32번 이슈: 차이 잉원 대만 총통\n",
      "33번 이슈: 경기 침체 우려 커지\n",
      "34번 이슈: 너지 솔루션 재생 에너지\n",
      "35번 이슈: 생명 과학 항서 제약\n",
      "36번 이슈: 과학 기반 감축 이니셔티브\n",
      "37번 이슈: 서부 텍사스 원유 배럴\n",
      "38번 이슈: 청년 연계 내일 공제\n",
      "39번 이슈: 박순애 부총리 교육부 장관\n",
      "40번 이슈: 카카오 게임즈 우마 무스메\n",
      "41번 이슈: 인터 오리지널 스톰 트루\n",
      "42번 이슈: 케이블 시청 종료 인터넷\n",
      "43번 이슈: 권성동 대표 직무 대행 원내대표\n",
      "44번 이슈: 본사 충북 증평 공장 방문\n",
      "45번 이슈: 초등 학교 입학 연령 낮추\n",
      "46번 이슈: 매출 둔화 직영 고정비 부담\n",
      "47번 이슈: 삼성전자 무인 공장 도입 추진\n",
      "48번 이슈: 비스 포크 무풍 에어컨 갤러리\n",
      "49번 이슈: 자회사 스타 키스트 감소 매출\n",
      "50번 이슈: 뷰티 건강 기능 식품 상품\n",
      "51번 이슈: 초록 기술 조합 버킷 스튜디오\n",
      "52번 이슈: 그린필드 스마트 시티 마스터 모델\n",
      "53번 이슈: 보합세 외환 환율 하락세 일본\n",
      "54번 이슈: 대상 배정 유상 증자 결정\n",
      "55번 이슈: 선적 공간 운임 할인 혜택\n",
      "56번 이슈: 카카오 대표 실적 발표 컨퍼런스\n",
      "57번 이슈: 인천 왕길 주택 신축 공사\n",
      "58번 이슈: 인치 고인 승용차 타이어 판매\n",
      "59번 이슈: 상장 격성 실질 심사 사유\n",
      "60번 이슈: 알츠하이머 치매 치료제 도네 리온\n",
      "61번 이슈: 퓨리 케어 오브제 컬렉션 정수기\n",
      "62번 이슈: 대만 직항 노선 운영 대한항공\n",
      "63번 이슈: 스튜디오 산타클로스 계열사 편입 제작\n",
      "64번 이슈: 회차 무기명식 권부 사모 전환사채\n",
      "65번 이슈: 비교 시험 계획 신청 예정\n",
      "66번 이슈: 겨울 패션 상품 최대 할인\n",
      "\n",
      "\n",
      "----- FINISHED -----\n"
     ]
    }
   ],
   "source": [
    "duplicates = []\n",
    "a = len(first_week3)\n",
    "for i in range(len(first_week3)):\n",
    "    dupll = []\n",
    "    for j in range(len(first_week3)):\n",
    "        if i < j:\n",
    "            dupl = list(set(first_week3[i][0].split()) & set(first_week3[j][0].split()))\n",
    "            if len(dupl) >= 2:\n",
    "                duplicates.append(j)\n",
    "\n",
    "res = []\n",
    "[res.append(x) for x in duplicates if x not in res]\n",
    "\n",
    "new_first_week = [val for n, val in enumerate(first_week3) if n not in res]\n",
    "\n",
    "def duple_check(lists):\n",
    "    for elem in lists:\n",
    "        if lists.count(elem) > 1:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "\n",
    "this_week_issue = []\n",
    "for i in range(len(new_first_week)-1):\n",
    "    result = duple_check(new_first_week[i][0].split())\n",
    "    if result:\n",
    "        pass\n",
    "    else:\n",
    "        if new_first_week[i][1] >= 3:\n",
    "            if new_first_week[i][0].split()[len(new_first_week[0][0].split())-1] == new_first_week[i+1][0].split()[0]:\n",
    "                \n",
    "                this_week_issue.append(' '.join(list((new_first_week[i][0].split()[:-1] + new_first_week[i+1][0].split()))))\n",
    "            else:\n",
    "\n",
    "                this_week_issue.append(new_first_week[i][0])\n",
    "                \n",
    "final_issue = []\n",
    "for j in range(len(this_week_issue)):\n",
    "    this_week_issues = []\n",
    "    for word in this_week_issue[j].split():\n",
    "        if word not in this_week_issues:\n",
    "            this_week_issues.append(word)\n",
    "        if word in this_week_issue:\n",
    "            pass\n",
    "    final_issue.append(this_week_issues)\n",
    "\n",
    "print('----- HERE ARE THE FREQUENT N-GRAMS -----')\n",
    "print('\\n')\n",
    "for i in range(len(final_issue)):\n",
    "    print(f'{i+1}번 이슈: ' + ' '.join(final_issue[i]))\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "print('----- FINISHED -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "51a4b038-253a-4876-98ad-d3a0f8432580",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "몇 번째 이슈에 대한 기사를 찾아볼까요?:  47\n"
     ]
    }
   ],
   "source": [
    "issue_num = int(input('몇 번째 이슈에 대한 기사를 찾아볼까요?: '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "e7e2fe61-8917-49ac-9e0a-9ce7654551c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1380                  삼성전자 무인공장 도입 호재…\"사명에 '로봇' 있으면 다 올라\"\n",
       "1595       [특징주]자비스, 세계최초 무인 자동방식 삼성전자 사업장 반도체 패키지 검사장비 적\n",
       "1740                  생산 공정 100% 자동화…삼전 '무인공장' 도입說에 로봇주급등\n",
       "1774    [주식 초고수는 지금] '한국판 록히드마틴' 꿈 영그는 한화에어로스페이스, 투자자 ...\n",
       "1804                  [마켓프로] 초고수의 선택…'로봇 테마' 인탑스·레인보우로보틱스\n",
       "1868                        삼성전자 '무인공장' 도입 보도에…유일로보틱스 상한가\n",
       "1890                                [한경라씨로] 유일로보틱스 상한가 가나\n",
       "1911          [특징주] 이삭엔지니어링, 국내 유일 스마트팩토리 종합 솔루션 기대감에 상승세\n",
       "1953        [특징주] 케이피에프, 삼성 지원 '스마트공장 지원사업 1호 선정' 부각 '강세'\n",
       "1964                [SEN]엠투아이, 삼성전자가 찾는 무인공정 기업 부각···강세 ?\n",
       "1992    [특징주]에스넷, 삼성전자 2030년 무인공장 도입 추진... 삼성 생산라인 구축 ...\n",
       "2002     [특징주]엠투아이, 삼성 2030년 '무인공장' 도입…스마트팩토리 핵심장비 국내 1위↑\n",
       "2026                      삼성전자 무인공장 도입 소식에 국내 로봇株 장 초반 강세\n",
       "2058                        [특징주] 삼성전자 무인공장 도입 소식에 로봇株 강세\n",
       "2060                            삼성전자 무인공장 도입 보도에 로봇株 '들썩'\n",
       "2078                      [특징주] TPC, 삼성전자 무인공장 도입 소식에 상승세\n",
       "2082     [특징주]에스피시스템스, 삼성전자 '100% 무인공장' 도입 추진에 자동화 시스템 구축\n",
       "2097                           [특징주] 삼전 공장 무인화 추진에 로봇株 강세\n",
       "2123    [특징주]티로보틱스, 삼성전자 무인공장 도입 추진...공정 100% 자동화 TF 구...\n",
       "3204             [특징주] THQ, 삼성 무인공장 도입↑...휴림로봇과 로봇대표기업 육성\n",
       "3407            무인 생산 시대 온다? 삼전 글로벌 생산기지 무인화 소식에 로봇주 '방긋'\n",
       "3630           교육·로봇 이어 제과주까지...상승 동력 없는 시장, 테마주만 번갈아 급등락\n",
       "3820              [도전 3/6/9] 이삭엔지니어링·레몬·선익시스템· NHN한국사이버결제\n",
       "Name: n_title, dtype: object"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ss = df[df['n_content_cleaned'].apply(lambda x: all(k in x for k in final_issue[issue_num-1]))]\n",
    "df_ss['n_title']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9520772-10c2-4156-b3f6-4a7a96d68136",
   "metadata": {},
   "source": [
    "# Title to Contents - Specific N-Grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "734a8330-c327-45e1-ab90-f9c4e806da3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- DATA PREPROCESSED -----\n"
     ]
    }
   ],
   "source": [
    "df_ss = df_ss.reset_index(drop = True)\n",
    "tokenized, pos = tokenize_and_postag(df_ss, 'n_content_cleaned')\n",
    "sw = pd.read_csv('sw.txt', delimiter = '\\n', header = None)\n",
    "sw = list(sw[0])\n",
    "df_ss['tokenized_content'] = tokenized\n",
    "df_ss['pos_tagged_content'] = pos\n",
    "\n",
    "# cleans = clean_word(df_ss, 'pos_tagged_content')\n",
    "cleans = clean_word(df_ss, 'pos_tagged_content', sw)\n",
    "print('----- DATA PREPROCESSED -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "fdfd0079-2e9f-4741-af4d-a0108602b806",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----- HERE ARE THE RELEVANT N-GRAMS -----\n",
      "\n",
      "\n",
      "경기 침체 우려\n",
      "보스 치치 수석 코노 미스트\n",
      "수석 코노 미스트\n",
      "거시경제 추경호 부총리\n",
      "기획 재정부 장관\n",
      "비바리 퍼블 리카\n",
      "카카오 페이 토스\n",
      "생애 최초 주택\n",
      "적용 한도 기존\n",
      "정부 규제 완화\n",
      "찰스 에번스 시카고\n",
      "신용 스프 레드\n",
      "중견 자금 조달\n",
      "통화 정책 확실\n",
      "강력 수치 당초\n",
      "강력 임금 강화 국채 포인트\n",
      "강화 국채 포인트\n",
      "강화 전망 웃돌\n",
      "거래소 그룹 페드\n",
      "견고 인플레이션 지표\n",
      "그간 인플레이션 정점\n",
      "노동부 농업 부문\n",
      "다우존스 전망 웃도\n",
      "단행 웃도 강력\n",
      "당초 발표 상향\n",
      "동월 오르 최고\n",
      "둔화 뒤엎 수치\n",
      "둔화 조짐 위원회\n",
      "머무르 준비 제도 인상\n",
      "\n",
      "\n",
      "----- FINISHED -----\n"
     ]
    }
   ],
   "source": [
    "doc_list = get_doc_list(df_ss, cleans)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 3000, ngram_range = (3, 3))\n",
    "tri_tfidf = get_tfidf_ngram(tfidf, df_ss, cleans, doc_list, 0.1)\n",
    "df_ss['tri_tfidf'] = tri_tfidf\n",
    "first_week3 = get_most_frequent(df_ss, 'tri_tfidf', 50)\n",
    "\n",
    "tfidf = TfidfVectorizer(max_features = 3000, ngram_range = (4, 4))\n",
    "tri_tfidf = get_tfidf_ngram(tfidf, df_ss, cleans, doc_list, 0.1)\n",
    "df_ss['tri_tfidf'] = tri_tfidf\n",
    "first_week1 = get_most_frequent(df_ss, 'tri_tfidf', 50)\n",
    "    \n",
    "tfidf = TfidfVectorizer(max_features = 3000, ngram_range = (5, 5))\n",
    "tri_tfidf = get_tfidf_ngram(tfidf, df_ss, cleans, doc_list, 0.1)\n",
    "df_ss['tri_tfidf'] = tri_tfidf\n",
    "first_week2 = get_most_frequent(df_ss, 'tri_tfidf', 50)\n",
    "\n",
    "\n",
    "first_week =  first_week3 + first_week1 + first_week2\n",
    "duplicates = []\n",
    "a = len(first_week)\n",
    "for i in range(len(first_week)):\n",
    "    dupll = []\n",
    "    for j in range(len(first_week)):\n",
    "        if i < j:\n",
    "            dupl = list(set(first_week[i][0].split()) & set(first_week[j][0].split()))\n",
    "            if len(dupl) >= 2:\n",
    "                duplicates.append(j)\n",
    "\n",
    "res = []\n",
    "[res.append(x) for x in duplicates if x not in res]\n",
    "\n",
    "new_first_week = [val for n, val in enumerate(first_week) if n not in res]\n",
    "\n",
    "def duple_check(lists):\n",
    "    for elem in lists:\n",
    "        if lists.count(elem) > 1:\n",
    "            return True\n",
    "        return False\n",
    "        \n",
    "print(f'----- HERE ARE THE RELEVANT N-GRAMS -----')\n",
    "print('\\n')\n",
    "for i in range(len(new_first_week)-1):\n",
    "    result = duple_check(new_first_week[i][0].split())\n",
    "    if result:\n",
    "        pass\n",
    "    else:\n",
    "        if new_first_week[i][1] >= 1:\n",
    "            if new_first_week[i][0].split()[len(new_first_week[0][0].split())-1] == new_first_week[i+1][0].split()[0]:\n",
    "                print(' '.join(list((new_first_week[i][0].split()[:-1] + new_first_week[i+1][0].split()))))\n",
    "            else:\n",
    "                print(new_first_week[i][0])\n",
    "print('\\n')\n",
    "print(f'----- FINISHED -----')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "49987877-2173-40da-be18-eb25c45a3762",
   "metadata": {},
   "outputs": [],
   "source": [
    "ogs = []\n",
    "\n",
    "for k in range(len(new_first_week)):\n",
    "    og = []\n",
    "    for i in range(len(new_first_week[k][0].split())):\n",
    "        for j in range(len(df_ss['n_title'][0].split(' '))):\n",
    "            if new_first_week[k][0].split()[i] in df_ss['n_title'][0].split(' ')[j]:\n",
    "                og.append(df_ss['n_title'][0].split(' ')[j])\n",
    "                break\n",
    "            else:\n",
    "                if (j+1) == len(df_ss['n_title'][0].split(' ')):\n",
    "                    og.append(new_first_week[k][0].split()[i])\n",
    "                else:\n",
    "                    pass\n",
    "                                                                         \n",
    "                                                                         \n",
    "\n",
    "    if len(og) > len(set(og)):\n",
    "        ogs.append(list(set(og)))\n",
    "    else:\n",
    "        ogs.append(og)\n",
    "    \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "427021cb-34b5-4d52-9738-24985f95b944",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['셀트리온제약,', '붙이는', '치매치료제', \"'도네리온패취'\", '국내', '판매']"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_ss['n_title'][0].split(' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "740c0e7d-000a-49e5-9c34-689eaf402880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['셀트리온제약,'],\n",
       " ['국내', '판매'],\n",
       " ['치매치료제'],\n",
       " [\"'도네리온패취'\", '셀트리온제약,'],\n",
       " ['셀트리온제약,', '국내'],\n",
       " ['치매치료제', \"'도네리온패취'\"],\n",
       " ['알츠하이머', '치매치료제'],\n",
       " ['셀트리온제약,', '알츠하이머'],\n",
       " ['판매', '개시'],\n",
       " ['셀트리온제약,', '붙이는'],\n",
       " ['붙이는', '알츠하이머'],\n",
       " ['치매치료제', '국내'],\n",
       " ['붙이는', '치매치료제'],\n",
       " [\"'도네리온패취'\", '패치'],\n",
       " ['셀트리온제약,', '치매치료제'],\n",
       " ['치매치료제', \"'도네리온패취'\"],\n",
       " ['패치', '국내'],\n",
       " ['알츠하이머', '치매치료제'],\n",
       " ['큐어', '셀트리온제약,']]"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "a561c777-de18-45c4-8aa6-a1e3abbf59d6",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60997/520357268.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnew_first_week\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mogs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "new_first_week[0][0] = ogs[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7ca3047a-6672-4eba-b98c-6c4ca56d9cd4",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tuple' object does not support item assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_60997/3675610350.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mnew_first_week\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_content_cleaned'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0mog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'n_content_cleaned'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m' '\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m             \u001b[0mnew_first_week\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: 'tuple' object does not support item assignment"
     ]
    }
   ],
   "source": [
    "for j in range(len(new_first_week)):\n",
    "    for i in range(len(df_ss['n_content_cleaned'][0].split(' '))):\n",
    "        og = []\n",
    "        if new_first_week[j][0].replace(' ', '') in df_ss['n_content_cleaned'][0].split(' ')[i]:\n",
    "            og.append(df_ss['n_content_cleaned'][0].split(' ')[i])\n",
    "            new_first_week[j][0] = min(og, key = len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "eae8f019-5e70-4935-9830-4dc4d53c863a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['트리', '온제']"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_first_week[0][0].split()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "718c1b02-0cf6-4a72-b19d-ba6e97347156",
   "metadata": {},
   "source": [
    "# When Data is Already Preprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fff3c0e2-9e87-4a8c-9918-d8c08a3b82c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('fw_preprocessed.csv')\n",
    "df['pos_tagged_content'] = df['pos_tagged_content'].apply(lambda k: eval(''.join(k)))\n",
    "sw = pd.read_csv('sw.txt', delimiter = '\\n', header = None)\n",
    "sw = list(sw[0])\n",
    "cleans = clean_word(df, 'pos_tagged_content', sw)\n",
    "doc_list = get_doc_list(df, cleans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "282d662b-3770-4c02-ab7c-0ebe40f84e81",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfVectorizer(max_features = 1000, ngram_range = (4, 5))\n",
    "tri_tfidf = get_tfidf_ngram(tfidf, df, cleans, doc_list, 0.8)\n",
    "df['tri_tfidf'] = tri_tfidf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "67a864a8-f4c3-40f1-9c54-e42b1d5bcf7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week = get_most_frequent(df, 'tri_tfidf', 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4bf281-9b4b-4c89-830d-1ee9a69b1c07",
   "metadata": {},
   "outputs": [],
   "source": [
    "first_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f4f2be-b3d9-47c8-9765-0c3bbe37e107",
   "metadata": {},
   "outputs": [],
   "source": [
    "duplicates = []\n",
    "a = len(first_week)\n",
    "for i in range(len(first_week)):\n",
    "    dupll = []\n",
    "    for j in range(len(first_week)):\n",
    "        if i < j:\n",
    "            dupl = list(set(first_week[i][0].split()) & set(first_week[j][0].split()))\n",
    "            if len(dupl) >= 3:\n",
    "                duplicates.append(j)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a5f2b9fc-82d8-4553-bfe4-ea92373208d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[3, 6, 7, 12, 19, 20, 32, 40, 41, 14, 46]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res = []\n",
    "[res.append(x) for x in duplicates if x not in res]\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2241166c-1758-4142-8650-d2862240076e",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_first_week = [val for n, val in enumerate(first_week) if n not in res]\n",
    "new_first_week"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "989c1b2f-d621-4922-b9ae-fbd5a1b7c4b6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "news",
   "language": "python",
   "name": "news"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
